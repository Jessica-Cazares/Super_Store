# ETL - Extracci√≥n, Transformaci√≥n y Carga :chart_with_upwards_trend: :arrows_counterclockwise:

# Super Store :convenience_store:

Super Store, l√≠der en el sector minorista, se enfrenta al desaf√≠o de gestionar grandes vol√∫menes de datos dispersos. Para mejorar la toma de decisiones, se propone la implementaci√≥n de un s√≥lido sistema ETL con tablas de hecho y dimensiones. Este proyecto busca crear un sistema integral que extraiga, transforme y cargue datos eficientemente, proporcionando una estructura jer√°rquica para facilitar el an√°lisis. El objetivo no es solo optimizar el almacenamiento, sino potenciar la capacidad de Super Store para identificar patrones y oportunidades de mercado, adapt√°ndose √°gilmente a cambios en la demanda del consumidor.

# Temas :bulb:

- [Introducci√≥n](#Introducci√≥n)
- [Objetivo](#Objetivo)
- [Procesamiento y an√°lisis](#Procesamiento-y-an√°lisis)
- [Herramientas](#Herramientas)
- [Lenguajes](#Lenguajes)
- [Resultados y Conclusiones](#Resultados-y-Conclusiones)
- [Ficha t√©cnica](/Ficha_t√©cnica/README.md)
- [Google Colab](/Google_Colab/README.md)
- [Dashboard](/Dashboard/README.md)
- [Presentaci√≥n](/Presentaci√≥n/README.md)

## Introducci√≥n 

ETL se refiere a un proceso de tres fases: Extracci√≥n (Extraction), Transformaci√≥n (Transformation) y Carga (Load). Este proceso se utiliza com√∫nmente en el √°mbito de la gesti√≥n y an√°lisis de datos, especialmente en el contexto de data warehouses y business intelligence.

__1. Extracci√≥n (Extraction):__ Durante esta fase, los datos se extraen desde una o varias fuentes de datos, que pueden ser bases de datos, archivos planos, servicios web u otras fuentes. La extracci√≥n implica recopilar la informaci√≥n necesaria para su posterior procesamiento.

__2. Transformaci√≥n (Transformation):__ En esta etapa, los datos extra√≠dos se transforman seg√∫n los requisitos del sistema de destino. Las transformaciones pueden incluir limpieza de datos, conversi√≥n de formatos, combinaci√≥n de datos de m√∫ltiples fuentes, filtrado y otras operaciones que aseguran que los datos sean coherentes y √∫tiles para el an√°lisis.

__3. Carga (Load):__ La fase final implica cargar los datos transformados en el sistema de destino, que generalmente es un data warehouse o una base de datos dise√±ada para el an√°lisis de negocios. Los datos ahora est√°n listos para ser consultados y analizados de manera eficiente.

## Objetivo

A trav√©s del proceso ETL (Extract, Transform, Load), construir un sistema tabular que nos permita almacenar datos de manera eficiente y consultar estos datos m√°s f√°cilmente.

## Procesamiento y an√°lisis

__üì• 1. Conectar/importar datos a otras herramientas__
- Se cre√≥ el proyecto `proyecto5-etl` y el conjunto de datos `Dataset` en BigQuery.
- Tablas importadas: **superstore**.

__üïµÔ∏è‚Äç‚ôÄÔ∏è 2. Valores nulos__
- Identificaci√≥n de valores nulos utilizando comandos SQL:
  - `COUNTIF`, `IS NULL`, `AS`.

__üóÉÔ∏è 3. Valores duplicados__
- Identificaci√≥n de registros duplicados utilizando comando SQL:
  - `COUNT`, `GROUP BY`, `HAVING`.

__üìã 4. Datos discrepantes en variables categ√≥ricas__
- Identificaci√≥n de datos discrepantes en variables categ√≥ricas mediante comandos SQL:
  - `DISTINCT`, `ORDER BY`.

__üìä 5. Datos discrepantes en variables num√©ricas__
- Identificaci√≥n de datos discrepantes en variables num√©ricas utilizando comandos SQL:
  - `COUNT`, `ARRAY_AGG`, `CAST`.

__üîÑ 6. Comprobar y cambiar tipo de dato__
- Se identific√≥ que las variables `ship_date` y `order_date` tienen el tipo de dato `TIMESTAMP`, es recomendable usar el tipo de dato `DATE`.

__üåê 7. Buscar datos de otras fuentes__
- Se extrajo informaci√≥n de la tabla "multinacional" de Wikipedia utilizando Python en [Google Colab](/Google_Colab/proyecto5_etl.ipynb).

__üõ†Ô∏è 8. Dise√±ar estructura de la base de datos (tablas de hechos y dimensiones)__
- Se dise√±√≥ un modelo de **esquema estrella** para la base de datos. 
- Clasificaci√≥n de las variables:
  - **Dimensiones üìè:** Variables que describen atributos contextuales (por ejemplo, productos, clientes, fechas).
  - **Hechos üìä:** Variables que contienen datos cuantitativos o eventos, como ventas, cantidad, y precios.
- Para ver el dise√±o de la estructura de la base de datos, puedes revisar [Ficha t√©cnica](/Ficha_t√©cnica/tablas.png).
    
__üóÑÔ∏è 9. Crear estructura de la base de datos (tablas de hechos y dimensiones)__
- Utilizando los comandos `CREATE TABLE`, `SELECT`, `DISTINCT`, `JOIN`, se crearon y llenaron las tablas en BigQuery.
  
- **Tablas de dimensiones** üìê: 
  - `customer_dim`
  - `category_dim`
  - `subcategory_dim`
  - `product_dim`
  - `order_dim`
  - `market_dim`
    
- **Tabla de hechos** üìä: 
  - `sales_superstore`

__‚è∞ 10. Programar actualizaciones de tablas__

- Se cre√≥ un **flujo de actualizaci√≥n** de tablas considerando las relaciones entre las tablas de hechos y dimensiones.
- El objetivo es actualizar los datos de manera eficiente y mantener la **integridad referencial** de las tablas.

__üîó 11. Unir tablas__

- Con los comandos `SELECT`, `LEFT JOIN`, se cre√≥ una tabla para el **an√°lisis exploratorio** üìä.
- La tabla se gener√≥ a partir de la estructura de tablas de dimensiones y de hechos, seleccionando variables que contienen informaci√≥n relevante.

Para ver m√°s detalles de cada uno de los pasos del procesamiento, puedes revisar [Ficha t√©cnica](/Ficha_t√©cnica/Ficha_t√©cnica.pdf).

__üîç 12. An√°lisis exploratorio__

- **Agrupar datos**: Seg√∫n variables categ√≥ricas para identificar patrones y tendencias.
- **Visualizar las variables categ√≥ricas**: Se crearon gr√°ficos y diagramas para obtener una mejor comprensi√≥n de los datos.
- **Aplicar medidas de tendencia central y dispersi√≥n**: Se calcularon estad√≠sticas como la media, mediana, rango y desviaci√≥n est√°ndar.
- **Visualizar distribuci√≥n**: Se crearon diagramas de dispersi√≥n.
- **Visualizar el comportamiento de los datos a lo largo del tiempo**: Se crearon gr√°ficos para observar datos a trav√©s de los a√±os.

Para ver m√°s detalles del an√°lisis exploratorio puedes revisar: [Dashboard](/Dashboard/Proyecto5_etl.pdf).

üîó https://lookerstudio.google.com/reporting/58c871d4-115c-4fec-b35d-976aae6fcd3a 

## Herramientas

* Google BigQuery
* Google Looker Studio
* Google Docs
* Google Slide
* Google Colab

## Lenguajes

* SQL
* Python

## Resultados y Conclusiones

* La creaci√≥n del **esquema de datos** basado en tablas de hechos y dimensiones permitir√° centralizar toda la informaci√≥n relevante de ventas, productos, clientes y mercados en un solo lugar, facilitando el an√°lisis de la informaci√≥n de forma precisa y r√°pida.

* La __tablas de hechos__ `sales_superstore` almacena grandes vol√∫menes de datos transaccionales, mientras que las __tablas de dimensiones,__ (`customers_dim`,`products_dim`, `markets_dim`, `order_dim`), contienen informaci√≥n descriptiva. Este dise√±o permite realizar consultas optimizadas y obtener resultados de manera m√°s eficiente.

* Con el dise√±o de tablas de hechos y dimensiones, se podr√°n realizar **an√°lisis a nivel de detalle y en diferentes dimensiones**, como analizar ventas por producto, cliente, segmento, mercado, y per√≠odo de tiempo.

* La implementaci√≥n de un **pipeline de actualizaci√≥n** permitir√° que las tablas se mantengan actualizadas de manera regular (por ejemplo, diariamente o semanalmente) con la √∫ltima informaci√≥n, asegurando la disponibilidad de datos recientes para la toma de decisiones.

* El dise√±o de tablas de hechos y dimensiones sigue un enfoque de **esquema estrella**, donde, la **tabla de hechos**(`sales_superstore`) se conecta con m√∫ltiples **tablas de dimensiones** (`customers_dim`, `products_dim`, `orders_dim`, `markets_dim`). Este esquema es ideal para el an√°lisis y la creaci√≥n de dashboards, ya que permite acceder a los datos de forma simple y r√°pida.

* La tabla de hechos `sales_superstore` tiene como **clave primaria** el `id_ticket`, lo que garantiza la unicidad de las transacciones. Las **llaves for√°neas** conectan la tabla de hechos con las tablas de dimensiones, asegurando que se pueda hacer un an√°lisis detallado utilizando los atributos de las dimensiones.

* El proceso de **ETL** debe estar dise√±ado para:
    - **Extraer** datos de las fuentes originales.
    - **Transformar** los datos adecuadamente (limpieza, c√°lculo de indicadores, generaci√≥n de claves, etc.).
    - **Cargar** los datos en las tablas del esquema estrella.

* El pipeline de actualizaci√≥n debe manejar estos pasos autom√°ticamente, evitando errores manuales y garantizando la integridad de los datos. La implementaci√≥n del pipeline de actualizaci√≥n permitir√° actualizar las tablas de hechos y dimensiones peri√≥dicamente, asegurando que los datos reflejen la situaci√≥n actual del negocio.

Para m√°s detalles sobre las conclusiones y recomendaciones, puedes revisar: [Ficha t√©cnica](/Ficha_t√©cnica/Ficha_t√©cnica) y [Presentaci√≥n](/Presentaci√≥n/Presentacion.pdf).

